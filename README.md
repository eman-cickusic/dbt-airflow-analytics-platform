# Production-Grade Customer 360 Data Warehouse

This project implements a complete, end-to-end ELT pipeline that builds a Customer 360 data warehouse from disparate raw data sources. The entire platform is fully containerized with Docker and orchestrated with Airflow, featuring data transformations and testing with dbt.

This system mirrors a modern, production-grade data stack and demonstrates best practices in data engineering, including:
*   **Declarative Infrastructure:** The entire stack is defined in a single `docker-compose.yml` file for one-command reproducibility.
*   **Orchestration:** Airflow is used to schedule and monitor the data ingestion pipeline, ensuring reliability and observability.
*   **ELT Architecture:** Raw data is first **Loaded** into a staging area and then **Transformed** in place using dbt, following modern analytics engineering principles.
*   **Data Modeling:** The final output is a clean, analytics-ready star schema (dimensional model) ready for BI or data science use cases.
*   **Automated Data Quality Testing:** The pipeline includes a dbt test suite to enforce data integrity, including checks for uniqueness, non-null values, and referential integrity.
*   **Integrated Documentation:** The project includes a self-generating documentation website that provides full data lineage and model descriptions.

## Architecture Diagram

<!-- 
  **ACTION REQUIRED:** Create an architecture diagram using a tool like draw.io or Excalidraw.
  Export it as a PNG or SVG and place it in a top-level `assets` folder.
  Then, replace this comment block with: ![Architecture Diagram](assets/architecture.png)
-->

## Tech Stack

| Category              | Tool / Technology                               | Purpose                                                                   |
| --------------------- | ----------------------------------------------- | ------------------------------------------------------------------------- |
| **Containerization**  | Docker, Docker Compose                          | To create a fully isolated, reproducible development environment.         |
| **Orchestration**     | Apache Airflow                                  | To schedule, execute, and monitor the data ingestion pipeline (the ELT 'L'). |
| **Data Lake**         | MinIO                                           | S3-compatible object storage for raw, unstructured source data (CSV files). |
| **Data Warehouse**    | PostgreSQL                                      | Central repository for both the staging and production data schemas.      |
| **Transformation**    | dbt (Data Build Tool)                           | To transform, model, and test the data from staging into a clean star schema. |
| **Data Generation**   | Python (Faker)                                  | To generate realistic mock source data for users, orders, and tickets.    |

## Project Overview

The primary goal of this project is to create a **single source of truth** for customer analytics by consolidating data from three siloed systems: a user database, an e-commerce platform, and a support ticketing system.

The pipeline works in two main stages:

1.  **Airflow Ingestion:** An Airflow DAG is responsible for the "Load" step. It reliably loads raw CSV data from the MinIO data lake into a `staging` schema in the PostgreSQL warehouse. This process is designed to be idempotent and atomic.

2.  **dbt Transformation:** dbt takes over for the "Transform" step. It reads the raw data from the `staging` schema and executes a series of SQL models to clean, join, and aggregate the data, materializing the final, analytics-ready tables in a `production` schema.

## Data Lineage

The following lineage graph was automatically generated by dbt and shows the flow of data from raw sources to final analytical models.

<!-- 
  **ACTION REQUIRED:** Run the `dbt docs serve` command, navigate to the documentation site,
  and take a high-quality screenshot of the full project lineage graph.
  Place it in the `assets` folder and replace this comment block with: ![dbt Lineage Graph](assets/dbt_lineage.png)
-->

## How to Run the Project

### Prerequisites

*   Docker Desktop installed and running.
*   A local Python environment (for initial data generation).

### Quickstart & Local Development

1.  **Clone the Repository:**
    ```bash
    git clone <your-repo-url>
    cd faang_de_project
    ```

2.  **Generate Raw Data:**
    This one-time step creates the raw CSV files.
    ```bash
    python -m pip install Faker
    python scripts/generate_data.py
    ```

3.  **Build the Custom Docker Images:**
    This command builds the custom Airflow and dbt images from their respective Dockerfiles. This may take several minutes on the first run.
    ```bash
    docker compose build --no-cache
    ```

4.  **Initialize the Database & Launch Services:**
    These commands will initialize the Airflow database, create an admin user, and launch the full stack in the background.
    ```bash
    docker compose up -d
    docker compose run --rm airflow-webserver db migrate
    docker compose run --rm airflow-webserver users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
    ```

### Accessing Services

*   **Airflow UI:** `http://localhost:8080` (Login: `admin` / `admin`)
*   **MinIO UI:** `http://localhost:9001` (Login: `minioadmin` / `minioadmin`)
*   **PostgreSQL:** Connect via `localhost:5432` (User: `airflow`, Pass: `airflow`, DB: `airflow`)

### Running the Full ELT Pipeline

1.  **Upload Data & Set Up Connections:**
    *   Navigate to the MinIO UI, create a bucket named `data-lake`, and upload the three CSV files from the local `raw_data` directory.
    *   Navigate to the Airflow UI (`Admin -> Connections`) and create the `minio_s3` and `postgres_warehouse` connections as detailed in the project setup.

2.  **Trigger the Airflow Ingestion DAG:**
    *   In the Airflow UI, enable and trigger the `ingest_data_dag`. Wait for it to complete successfully.

3.  **Run the dbt Transformations & Tests:**
    *   From your terminal, execute the dbt commands via the container:
    ```bash
    # Run all models
    docker compose run dbt dbt run

    # Run all data quality tests
    docker compose run dbt dbt test
    ```

4.  **View the Documentation:**
    ```bash
    # Generate the documentation assets
    docker compose run dbt dbt docs generate

    # Serve the website
    docker compose run --publish 8081:8081 dbt dbt docs serve --host 0.0.0.0
    ```
    *   Navigate to `http://localhost:8081` to view the documentation site.
